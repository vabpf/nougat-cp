{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda3\\envs\\nougat\\Lib\\site-packages\\tqdm-4.67.0-py3.12.egg\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face cache directory: D:\\Models\\huggingface\n",
      "PyTorch cache directory: D:\\Models\\torch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "print(f\"Hugging Face cache directory: {os.getenv('HF_HOME')}\")\n",
    "print(f\"PyTorch cache directory: {os.getenv('TORCH_HOME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub = os.getenv('TORCH_HOME')\n",
    "from pathlib import Path\n",
    "# Path(hub).exists()\n",
    "hub.rename(hub.with_name(\"nougat-0.1.0-small\"))\n",
    "hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Models\\\\torch'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get(\"NOUGAT_CHECKPOINT\", hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "<Response [500]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://127.0.0.1:8503/predict/'\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "}\n",
    "files = {\n",
    "    'file': ('Hsg lớp 9 ứng hòa.pdf', open(r'C:/Users/admin/Downloads/Hsg lớp 9 ứng hòa.pdf', 'rb'), 'application/pdf')\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, files=files)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markdown\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: markdown\n",
      "Successfully installed markdown-3.7\n"
     ]
    }
   ],
   "source": [
    "%pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>\\begin{table}\n",
      "\\begin{tabular}{|p{113.8pt}|p{113.8pt}|} \\hline\n",
      "<strong>Method</strong> &amp; <strong>MAE</strong> \\ \\hline AMDCN <strong>(without perspective information)</strong> &amp; 16.6 \\ \\hline AMDCN (with perspective information) &amp; 14.9 \\ \\hline LBP+RR [28] (with perspective information) &amp; 31.0 \\ \\hline MCNN [28] (with perspective information) &amp; <strong>11.6</strong> \\ \\hline\n",
      "[27] (with perspective information) &amp; 12.9 \\ \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 4: Mean absolute error of various methods on WorldExpo crowds</p>\n"
     ]
    }
   ],
   "source": [
    "import markdown\n",
    "html = markdown.markdown(r\"\"\"\\begin{table}\n",
    "\\begin{tabular}{|p{113.8pt}|p{113.8pt}|} \\hline\n",
    "**Method** & **MAE** \\\\ \\hline AMDCN **(without perspective information)** & 16.6 \\\\ \\hline AMDCN (with perspective information) & 14.9 \\\\ \\hline LBP+RR [28] (with perspective information) & 31.0 \\\\ \\hline MCNN [28] (with perspective information) & **11.6** \\\\ \\hline\n",
    "[27] (with perspective information) & 12.9 \\\\ \\hline \\end{tabular}\n",
    "\\end{table}\n",
    "Table 4: Mean absolute error of various methods on WorldExpo crowds\"\"\")\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Internal Server Error'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import NougatProcessor, VisionEncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "processor = NougatProcessor.from_pretrained(\"facebook/nougat-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"facebook/nougat-base\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "# prepare PDF image for the model\n",
    "filepath = hf_hub_download(repo_id=\"hf-internal-testing/fixtures_docvqa\", filename=\"nougat_paper.png\", repo_type=\"dataset\")\n",
    "image = Image.open(filepath)\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# generate transcription (here we only generate 30 tokens)\n",
    "outputs = model.generate(\n",
    "    pixel_values.to(device),\n",
    "    min_length=1,\n",
    "    max_new_tokens=30,\n",
    "    bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    ")\n",
    "\n",
    "sequence = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "sequence = processor.post_process_generation(sequence, fix_markdown=False)\n",
    "# note: we're using repr here such for the sake of printing the \\n characters, feel free to just print the sequence\n",
    "print(repr(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda3\\envs\\nougat\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing packages: ['opencv_python_headless', 'bs4', 'Pebble', 'htmlmin', 'pdfminer']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "packages = [\n",
    "    \"transformers\", \"timm\", \"orjson\", \"opencv_python_headless\", \"datasets\", \"lightning\",\n",
    "    \"nltk\", \"Levenshtein\", \"sentencepiece\", \"sconf\", \"albumentations\", \"pypdf\", \"pypdfium2\",\n",
    "    \"fastapi\", \"uvicorn\", \"multipart\", \"pytesseract\", \"bs4\", \"sklearn\", \"Pebble\", \"pylatexenc\",\n",
    "    \"fuzzysearch\", \"unidecode\", \"htmlmin\", \"pdfminer\"\n",
    "]\n",
    "\n",
    "missing_packages = []\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(\"Missing packages:\", missing_packages)\n",
    "else:\n",
    "    print(\"All packages are installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = [\n",
    "    \"transformers\", \"timm\", \"orjson\", \"opencv_python_headless\", \"datasets\", \"lightning\",\n",
    "    \"nltk\", \"Levenshtein\", \"sentencepiece\", \"sconf\", \"albumentations\", \"pypdf\", \"pypdfium2\",\n",
    "    \"fastapi\", \"uvicorn\", \"multipart\", \"pytesseract\", \"bs4\", \"sklearn\", \"Pebble\", \"pylatexenc\",\n",
    "    \"fuzzysearch\", \"unidecode\", \"htmlmin\", \"pdfminer\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('requirements.txt', 'w') as f:\n",
    "    for package in packages:\n",
    "        f.write(f\"{package}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv_python_headless in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in d:\\apps\\anaconda3\\envs\\nougat\\lib\\site-packages (from opencv_python_headless) (1.26.4)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\n",
      "Requirement already satisfied: Pebble in d:\\apps\\anaconda3\\envs\\nougat\\lib\\site-packages (5.0.7)\n",
      "Collecting htmlmin\n",
      "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: htmlmin\n",
      "  Building wheel for htmlmin (setup.py): started\n",
      "  Building wheel for htmlmin (setup.py): finished with status 'done'\n",
      "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27091 sha256=f16c27f56194da01cc6b91f0728332e8b2c2a002b36b4cf28a32d29a6582d876\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\5f\\d4\\d7\\4189b07b5902ee9f3ce0dbb14909fbe8037c39d6c63ffd49c9\n",
      "Successfully built htmlmin\n",
      "Installing collected packages: htmlmin\n",
      "Successfully installed htmlmin-0.1.12\n",
      "Collecting pdfminer\n",
      "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
      "     ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 0.8/4.2 MB 8.3 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 2.1/4.2 MB 6.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 3.1/4.2 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.2/4.2 MB 5.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pycryptodome (from pdfminer)\n",
      "  Downloading pycryptodome-3.21.0-cp36-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pycryptodome-3.21.0-cp36-abi3-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.6/1.8 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 4.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pdfminer\n",
      "  Building wheel for pdfminer (setup.py): started\n",
      "  Building wheel for pdfminer (setup.py): finished with status 'done'\n",
      "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140148 sha256=961c9fe26e98fd510b1f2040bb319128c29de8e4364be181d54213ca42e69568\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\90\\7b\\26\\62139fb7c8c5c242c492e02ce8613ca4c3df4cd86afb8e6264\n",
      "Successfully built pdfminer\n",
      "Installing collected packages: pycryptodome, pdfminer\n",
      "Successfully installed pdfminer-20191125 pycryptodome-3.21.0\n"
     ]
    }
   ],
   "source": [
    "for i in missing_packages:\n",
    "    %pip install {i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# # Read the JSON file\n",
    "# with open(r\"D:\\Data\\texify_bench.json\\bench_data.json\", 'r') as json_file:\n",
    "#     data = json.load(json_file)\n",
    "\n",
    "# # Write to the JSONL file\n",
    "# with open('output.jsonl', 'w') as jsonl_file:\n",
    "#     for entry in data:\n",
    "#         jsonl_file.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'test/data/train.jsonl'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            json_obj = json.loads(line)\n",
    "            # Process the JSON object as needed\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Skipping line {i} due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\FPT\\\\math-ocr\\\\nougat\\\\test\\\\data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import json\n",
    "\n",
    "def save_base64_image(base64_str, output_dir, filename):\n",
    "    img_data = base64.b64decode(base64_str)\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(img_data)\n",
    "    return file_path\n",
    "\n",
    "def process_jsonl_file(input_file, output_dir):\n",
    "    output_file = input_file.replace('0.jsonl', '.jsonl')\n",
    "    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "        for i, line in enumerate(f_in):\n",
    "            try:\n",
    "                json_obj = json.loads(line)\n",
    "                if 'image' in json_obj:\n",
    "                    base64_str = json_obj['image']\n",
    "                    filename = f'image_{i}.png'\n",
    "                    file_path = save_base64_image(base64_str, output_dir, filename)\n",
    "                    file_path = file_path.replace('arxiv\\\\', '')\n",
    "                    json_obj['image'] = file_path\n",
    "                f_out.write(json.dumps(json_obj) + '\\n')\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping line {i} due to error: {e}\")\n",
    "\n",
    "# Define the directories and files\n",
    "output_dir = 'arxiv'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "train_file = 'train0.jsonl'\n",
    "validation_file = 'validation0.jsonl'\n",
    "test_file = 'test0.jsonl'\n",
    "\n",
    "# Process the files\n",
    "process_jsonl_file(train_file, output_dir)\n",
    "process_jsonl_file(validation_file, output_dir)\n",
    "process_jsonl_file(test_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Data\\\\vietnamese_ocr_data'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'D:\\Data\\vietnamese_ocr_data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "def combine_images_and_text(equation_image_dir, text_image_dir, output_dir, jsonl_file):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    equation_image_files = sorted([f for f in os.listdir(equation_image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))])\n",
    "    text_image_files = sorted([f for f in os.listdir(text_image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))])\n",
    "\n",
    "    num_pairs = min(len(equation_image_files), len(text_image_files))\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        eq_img_path = os.path.join(equation_image_dir, equation_image_files[i])\n",
    "        text_img_path = os.path.join(text_image_dir, text_image_files[i])\n",
    "\n",
    "        try:\n",
    "            eq_img = Image.open(eq_img_path)\n",
    "            text_img = Image.open(text_img_path)\n",
    "\n",
    "            eq_text_file = os.path.splitext(equation_image_files[i])[0] + \".gt.txt\"\n",
    "            text_text_file = os.path.splitext(text_image_files[i])[0] + \".txt\"\n",
    "\n",
    "            with open(os.path.join(equation_image_dir, eq_text_file), 'r', encoding='utf-8') as f:\n",
    "                equation_text = f.read().strip() \n",
    "            with open(os.path.join(text_image_dir, text_text_file), 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "\n",
    "\n",
    "             # Left-Right Combination\n",
    "            combined_img_lr = Image.new('RGB', (eq_img.width + text_img.width, max(eq_img.height, text_img.height)), color='white')\n",
    "\n",
    "            # Top-Bottom Combination\n",
    "            combined_img_tb = Image.new('RGB', (max(eq_img.width, text_img.width), eq_img.height + text_img.height), color='white')\n",
    "\n",
    "            if random.choice([True, False]):\n",
    "                combined_img_lr.paste(eq_img, (0, 0))\n",
    "                combined_img_lr.paste(text_img, (eq_img.width, 0))\n",
    "                combined_text_lr = equation_text + \" \" + text\n",
    "            else:\n",
    "                combined_img_lr.paste(text_img, (0, 0))\n",
    "                combined_img_lr.paste(eq_img, (text_img.width, 0))\n",
    "                combined_text_lr = text + \" \" + equation_text\n",
    "\n",
    "            if random.choice([True, False]):\n",
    "                combined_img_tb.paste(text_img, (0, 0))\n",
    "                combined_img_tb.paste(eq_img, (0, text_img.height))\n",
    "                combined_text_tb = text + \"\\n\" + equation_text\n",
    "            else:\n",
    "                combined_img_tb.paste(eq_img, (0, 0))\n",
    "                combined_img_tb.paste(text_img, (0, eq_img.height))\n",
    "                combined_text_tb = equation_text + \"\\n\" + text\n",
    "\n",
    "            combined_lr_path = os.path.join(output_dir, f\"combined_lr_{i}.png\")\n",
    "            combined_tb_path = os.path.join(output_dir, f\"combined_tb_{i}.png\")\n",
    "            combined_img_lr.save(combined_lr_path)\n",
    "            combined_img_tb.save(combined_tb_path)\n",
    "\n",
    "            data.append({\"image\": combined_lr_path, \"equation\": combined_text_lr})\n",
    "            data.append({\"image\": combined_tb_path, \"equation\": combined_text_tb})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Text file not found for image {i}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing images {i}: {e}\")\n",
    "\n",
    "\n",
    "    # Save JSONL\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with open(os.path.join(output_dir, jsonl_file), 'w', encoding='utf-8') as jsonl_f:\n",
    "        for item in data:\n",
    "            jsonl_f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "\n",
    "equation_image_dir = r\"D:\\Data\\IM2LATEX-100K\\train\"\n",
    "text_image_dir = r\"D:\\Data\\vietnamese_ocr_data\\InkData_line_processed\"\n",
    "output_dir = \"combined_output\"\n",
    "jsonl_file = \"combined_data.jsonl\"\n",
    "\n",
    "\n",
    "combine_images_and_text(equation_image_dir, text_image_dir, output_dir, jsonl_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nougat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
